# -*- coding: utf-8 -*-
"""EM Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I3jAZe4EV8P-CzXjso3AttZUOSZ2cwCD

##EM Model on Iris dataset
"""

import numpy as np
from scipy.stats import multivariate_normal
from sklearn.datasets import load_iris

data = load_iris()
X = data.data
y = data.target

class GMM:

  def __init__(self, X, number_of_classes, iterations):
    self.iterations = iterations
    self.number_of_classes = number_of_classes
    self.X = X
    self.mu = None
    self.pi = None
    self.cov = None
    
  def run(self):

    # Set the initial mu, covariance and pi values
    self.reg_cov = 1e-6 * np.identity(len(self.X[0]))
    self.mu = np.random.randint(min(self.X[:,0]), max(self.X[:,0]), size = (self.number_of_classes, len(self.X[0])))
    self.pi = np.ones(self.number_of_classes) / self.number_of_classes
    self.cov = np.zeros((self.number_of_classes, len(X[0]), len(X[0]))) 
    for dim in range(len(self.cov)):
      np.fill_diagonal(self.cov[dim], 5)

    log_likelihoods = [] 
    for i in range(self.iterations):               

      # E step
      # Calculating prob. of each class for all data points
      prob_j = []
      for pi_j, mu_j, cov_j in zip(self.pi, self.mu, self.cov + self.reg_cov):
        prob_j.append(pi_j * multivariate_normal(mean = mu_j, cov = cov_j).pdf(X))
      
      # Sum of class probabilities
      sum_prob = np.sum(prob_j, axis=0)
      w_ij = np.zeros((len(self.X), len(self.cov)))
      for j in range(len(w_ij[0])):
        #co += self.reg_cov
        #nomin = p * multivariate_normal(mean = m,cov = co).pdf(self.X)
        #print(prob_j[0][0], ' / ', sum_prob[0], ' = ', prob_j[0][0] / sum_prob[0])
        w_ij[:,j] = prob_j[j] / sum_prob

      # M step
      self.mu = []
      self.cov = []
      self.pi = []
      log_likelihood = []

      for j in range(len(w_ij[0])):
        sum_j = np.sum(w_ij[:,j], axis=0)
        mu_j = (1 / sum_j) * np.sum(self.X * w_ij[:,j].reshape(len(self.X), 1), axis=0)
        self.mu.append(mu_j)
        self.cov.append(((1 / sum_j) * np.dot((np.array(w_ij[:,j]).reshape(len(self.X), 1) * (self.X - mu_j)).T, (self.X - mu_j))) + self.reg_cov)
        self.pi.append(sum_j / np.sum(w_ij))
    
      # Log likelihood
      prob_j = []
      for pi_j, mu_j, cov_j in zip(self.pi, self.mu, self.cov):
        prob_j.append(pi_j * multivariate_normal(mu_j, cov_j).pdf(X))
      
      log_likelihood = np.log(np.sum(prob_j))

      if len(log_likelihoods) > 0:
        if log_likelihoods[-1] > log_likelihood:
          log_likelihoods.append(log_likelihood)
          break

      log_likelihoods.append(log_likelihood)
  
  def predict(self, X):
    prediction = []  
    denom_j = []
    for mean, cov in zip(self.mu, self.cov):
      denom_j.append(multivariate_normal(mean = mean, cov = cov).pdf(X))

    denom = np.sum(denom_j)
    for m, c in zip(self.mu, self.cov):  
      nomin = multivariate_normal(mean = m, cov = c).pdf(X)
      prediction.append(nomin / denom)

    return prediction
        
# Testing 10 different instances of EM model
max_accs = []
for iter in range(0, 10):
  model = GMM(X, 3, 100)     
  model.run()

  # Testing accuracy
  results = np.array([])
  for x in X:
    result = model.predict(x)
    results = np.append(results, result.index(max(result)))

  # Fitting predictions to correct class
  accs = np.array([])
  accs = np.append(accs, sum(1 for x, y in zip(results, y) if x == y) / len(results))
  for m in range(0, 2):
    for n in range(0, 2):
      results = results - 1
      results[np.where(results == -1)[0]] = 2
      accs = np.append(accs, sum(1 for x, y in zip(results, y) if x == y) / len(results))

    # Swapping 0's and 2's to get other permutations
    if m == 0:
      results = results - 1
      results[np.where(results == 0)[0]] = 2
      results[np.where(results == -1)[0]] = 0
      accs = np.append(accs, sum(1 for x, y in zip(results, y) if x == y) / len(results))

  max_accs.append(max(accs))

print('Results over 10 EM model instances')
print('mean acc: ', np.sum(max_accs) / len(max_accs))
print('acc range: ', min(max_accs), '-', max(max_accs))

"""##Comparing model with sklearn EM model"""

from numpy import hstack
from numpy.random import normal
from sklearn.mixture import GaussianMixture
from sklearn.datasets import load_iris

data = load_iris()
X = data.data
y = data.target

# Testing 10 different instances of EM model
max_accs = []
for iter in range(0, 10):
  # fit model
  model = GaussianMixture(n_components=3, init_params='random')
  model.fit(X)

  # Testing accuracy
  results = np.array([])
  results = model.predict(X)

  accs = np.array([])
  accs = np.append(accs, sum(1 for x, y in zip(results, y) if x == y) / len(results))

  for m in range(0, 2):
    for n in range(0, 2):
      results = results - 1
      results[np.where(results == -1)[0]] = 2
      accs = np.append(accs, sum(1 for x, y in zip(results, y) if x == y) / len(results))

    # Swapping 0's and 2's to get other permutations
    if m == 0:
      results = results - 1
      results[np.where(results == 0)[0]] = 2
      results[np.where(results == -1)[0]] = 0
      accs = np.append(accs, sum(1 for x,y in zip(results,y) if x == y) / len(results))

  max_accs.append(max(accs))

print('Results over 10 EM modelings')
print('mean acc: ', np.sum(max_accs) / len(max_accs))
print('acc range: ', min(max_accs), '-', max(max_accs))

"""##Reference"""

# TODO: clean up unused imports
import matplotlib.pyplot as plt
from matplotlib import style
style.use('fivethirtyeight')
from sklearn.datasets.samples_generator import make_blobs
import numpy as np
from scipy.stats import multivariate_normal
from sklearn.datasets import load_iris

data = load_iris()
X = data.data
y = data.target
label_names = data.target_names

class GMM:

    def __init__(self,X,number_of_sources,iterations):
        self.iterations = iterations
        self.number_of_sources = number_of_sources
        self.X = X
        self.mu = None
        self.pi = None
        self.cov = None
        self.XY = None
        
    """Define a function which runs for iterations, iterations"""
    def run(self):
        self.reg_cov = 1e-6*np.identity(len(self.X[0]))

        """ 1. Set the initial mu, covariance and pi values"""
        # This is a nxm matrix since we assume n sources (n Gaussians) where each has m dimensions
        self.mu = np.random.randint(min(self.X[:,0]),max(self.X[:,0]),size=(self.number_of_sources,len(self.X[0])))

        # We need a nxmxm covariance matrix for each source since we have m features --> We create symmetric covariance matrices with ones on the digonal
        self.cov = np.zeros((self.number_of_sources,len(X[0]),len(X[0]))) 
        for dim in range(len(self.cov)):
            np.fill_diagonal(self.cov[dim],5)

        # Are "Fractions"
        self.pi = np.ones(self.number_of_sources)/self.number_of_sources

        # In this list we store the log likehoods per iteration and plot them in the end to check if
        # if we have converged
        log_likelihoods = [] 

        print(self.mu)
        
        # training model over iterations
        for i in range(self.iterations):               

            """E Step"""
            w_ic = np.zeros((len(self.X),len(self.cov)))
            for m,co,p,r in zip(self.mu,self.cov,self.pi,range(len(w_ic[0]))):
                co+=self.reg_cov
                mn = multivariate_normal(mean=m,cov=co)
                
                nomin = p*mn.pdf(self.X)
                denom = np.sum([pi_c*multivariate_normal(mean=mu_c,cov=cov_c).pdf(X) for pi_c,mu_c,cov_c in zip(self.pi,self.mu,self.cov+self.reg_cov)],axis=0)
            
                w_ic[:,r] = nomin / denom

            """
            The above calculation of w_ic is not that obvious why I want to quickly derive what we have done above.
            First of all the nominator:
            We calculate for each source c which is defined by m,co and p for every instance x_i, the multivariate_normal.pdf() value.
            For each loop this gives us a 100x1 matrix (This value divided by the denominator is then assigned to w_ic[:,r] which is in 
            the end a 100x3 matrix).
            Second the denominator:
            What we do here is, we calculate the multivariate_normal.pdf() for every instance x_i for every source c which is defined by
            pi_c, mu_c, and cov_c and write this into a list. This gives us a 3x100 matrix where we have 100 entrances per source c.
            Now the formula wants us to add up the pdf() values given by the 3 sources for each x_i. Hence we sum up this list over axis=0.
            This gives us then a list with 100 entries.
            What we have now is FOR EACH LOOP a list with 100 entries in the nominator and a list with 100 entries in the denominator
            where each element is the pdf per class c for each instance x_i (nominator) respectively the summed pdf's of classes c for each 
            instance x_i. Consequently we can now divide the nominator by the denominator and have as result a list with 100 elements which we
            can then assign to w_ic[:,r] --> One row r per source c. In the end after we have done this for all three sources (three loops)
            and run from r==0 to r==2 we get a matrix with dimensionallity 100x3 which is exactly what we want.
            If we check the entries of w_ic we see that there mostly one element which is much larger than the other two. This is because
            every instance x_i is much closer to one of the three gaussians (that is, much more likely to come from this gaussian) than
            it is to the other two. That is practically speaing, w_ic gives us the fraction of the probability that x_i belongs to class
            c over the probability that x_i belonges to any of the classes c (Probability that x_i occurs given the 3 Gaussians).
            """

            """M Step"""

            # Calculate the new mean vector and new covariance matrices, based on the probable membership of the single x_i to classes c --> w_ic
            self.mu = []
            self.cov = []
            self.pi = []
            log_likelihood = []

            for c in range(len(w_ic[0])):
                #sum of predicted labels (w_ji)
                m_c = np.sum(w_ic[:,c],axis=0)
                
                # new mu values for each feature
                mu_c = (1/m_c)*np.sum(self.X*w_ic[:,c].reshape(len(self.X),1),axis=0)
                
                self.mu.append(mu_c)

                # Calculate the covariance matrix per source based on the new mean
                self.cov.append(((1/m_c)*np.dot((np.array(w_ic[:,c]).reshape(len(self.X),1)*(self.X-mu_c)).T,(self.X-mu_c)))+self.reg_cov)
                
                # Calculate pi_new which is the "fraction of points" respectively the fraction of the probability assigned to each source 
                self.pi.append(m_c/np.sum(w_ic)) # Here np.sum(w_ic) gives as result the number of instances. This is logical since we know 
                                                # that the columns of each row of w_ic adds up to 1. Since we add up all elements, we sum up all
                                                # columns per row which gives 1 and then all rows which gives then the number of instances (rows) 
                                                # in X --> Since pi_new contains the fractions of datapoints, assigned to the sources c,
                                                # The elements in pi_new must add up to 1
          
            """Log likelihood"""
            summable = []
            for k,i,j in zip(self.pi,range(len(self.mu)),range(len(self.cov))):
              summable.append(k * multivariate_normal(self.mu[i],self.cov[j]).pdf(X))
            
            log_likelihood = np.log(np.sum(summable))
            log_likelihoods.append(log_likelihood)

        # Plotting Log likelihood
        fig2 = plt.figure(figsize=(10,10))
        ax1 = fig2.add_subplot(111) 
        ax1.set_title('Log-Likelihood')
        ax1.plot(range(0,self.iterations,1),log_likelihoods)
        plt.show()
    
    """Predict the membership of an unseen, new datapoint"""
    def predict(self, Y):
        prediction = []        
        for m,c in zip(self.mu,self.cov):  
            prediction.append(multivariate_normal(mean=m,cov=c).pdf(Y)/np.sum([multivariate_normal(mean=mean,cov=cov).pdf(Y) for mean,cov in zip(self.mu,self.cov)]))
        return prediction
         
max_accs = []
for iter in range(0, 10):
  model = GMM(X,3,100)     
  model.run()

  # Testing ACC
  results = np.array([])
  for x in X:
    result = model.predict(x)
    results = np.append(results, result.index(max(result)))

  accs = np.array([])
  accs = np.append(accs, sum(1 for x,y in zip(results,y) if x == y) / len(results))

  for m in range(0, 2):
    for n in range(0, 2):
      results = results - 1
      results[np.where(results == -1)[0]] = 2
      accs = np.append(accs, sum(1 for x,y in zip(results,y) if x == y) / len(results))

    # switching it up
    if i == m:
      results = results - 1
      results[np.where(results == 0)[0]] = 2
      results[np.where(results == -1)[0]] = 0
      accs = np.append(accs, sum(1 for x,y in zip(results,y) if x == y) / len(results))

  print(accs)
  print('Max acc: ', max(accs))
  max_accs.append(max(accs))

print('mean: ', np.sum(max_accs) / len(max_accs))
print('range: ', min(max_accs), '-', max(max_accs))