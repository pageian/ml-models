# -*- coding: utf-8 -*-
"""EM Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I3jAZe4EV8P-CzXjso3AttZUOSZ2cwCD

##EM Model on Iris dataset
"""

import numpy as np
from scipy.stats import multivariate_normal
from sklearn.datasets import load_iris

data = load_iris()
X = data.data
y = data.target

class GMM:

  def __init__(self, X, number_of_classes, iterations):
    self.iterations = iterations
    self.number_of_classes = number_of_classes
    self.X = X
    self.mu = None
    self.pi = None
    self.cov = None
    
  def run(self):

    # Set the initial mu, covariance and pi values
    self.reg_cov = 1e-6 * np.identity(len(self.X[0]))
    self.mu = np.random.randint(min(self.X[:,0]), max(self.X[:,0]), size = (self.number_of_classes, len(self.X[0])))
    self.pi = np.ones(self.number_of_classes) / self.number_of_classes
    self.cov = np.zeros((self.number_of_classes, len(X[0]), len(X[0]))) 
    for dim in range(len(self.cov)):
      np.fill_diagonal(self.cov[dim], 5)

    log_likelihoods = [] 
    for i in range(self.iterations):               

      # E step
      # Calculating prob. of each class for all data points
      prob_j = []
      for pi_j, mu_j, cov_j in zip(self.pi, self.mu, self.cov + self.reg_cov):
        prob_j.append(pi_j * multivariate_normal(mean = mu_j, cov = cov_j).pdf(X))
      
      # Sum of class probabilities
      sum_prob = np.sum(prob_j, axis=0)
      w_ij = np.zeros((len(self.X), len(self.cov)))
      for j in range(len(w_ij[0])):
        #co += self.reg_cov
        #nomin = p * multivariate_normal(mean = m,cov = co).pdf(self.X)
        #print(prob_j[0][0], ' / ', sum_prob[0], ' = ', prob_j[0][0] / sum_prob[0])
        w_ij[:,j] = prob_j[j] / sum_prob

      # M step
      self.mu = []
      self.cov = []
      self.pi = []
      log_likelihood = []

      for j in range(len(w_ij[0])):
        sum_j = np.sum(w_ij[:,j], axis=0)
        mu_j = (1 / sum_j) * np.sum(self.X * w_ij[:,j].reshape(len(self.X), 1), axis=0)
        self.mu.append(mu_j)
        self.cov.append(((1 / sum_j) * np.dot((np.array(w_ij[:,j]).reshape(len(self.X), 1) * (self.X - mu_j)).T, (self.X - mu_j))) + self.reg_cov)
        self.pi.append(sum_j / np.sum(w_ij))
    
      # Log likelihood
      prob_j = []
      for pi_j, mu_j, cov_j in zip(self.pi, self.mu, self.cov):
        prob_j.append(pi_j * multivariate_normal(mu_j, cov_j).pdf(X))
      
      log_likelihood = np.log(np.sum(prob_j))

      if len(log_likelihoods) > 0:
        if log_likelihoods[-1] > log_likelihood:
          log_likelihoods.append(log_likelihood)
          break

      log_likelihoods.append(log_likelihood)
  
  def predict(self, X):
    prediction = []  
    denom_j = []
    for mean, cov in zip(self.mu, self.cov):
      denom_j.append(multivariate_normal(mean = mean, cov = cov).pdf(X))

    denom = np.sum(denom_j)
    for m, c in zip(self.mu, self.cov):  
      nomin = multivariate_normal(mean = m, cov = c).pdf(X)
      prediction.append(nomin / denom)

    return prediction
        
# Testing 10 different instances of EM model
max_accs = []
for iter in range(0, 10):
  model = GMM(X, 3, 100)     
  model.run()

  # Testing accuracy
  results = np.array([])
  for x in X:
    result = model.predict(x)
    results = np.append(results, result.index(max(result)))

  # Fitting predictions to correct class
  accs = np.array([])
  accs = np.append(accs, sum(1 for x, y in zip(results, y) if x == y) / len(results))
  for m in range(0, 2):
    for n in range(0, 2):
      results = results - 1
      results[np.where(results == -1)[0]] = 2
      accs = np.append(accs, sum(1 for x, y in zip(results, y) if x == y) / len(results))

    # Swapping 0's and 2's to get other permutations
    if m == 0:
      results = results - 1
      results[np.where(results == 0)[0]] = 2
      results[np.where(results == -1)[0]] = 0
      accs = np.append(accs, sum(1 for x, y in zip(results, y) if x == y) / len(results))

  max_accs.append(max(accs))

print('Results over 10 EM model instances')
print('mean acc: ', np.sum(max_accs) / len(max_accs))
print('acc range: ', min(max_accs), '-', max(max_accs))

"""##Comparing model with sklearn EM model"""

from numpy import hstack
from numpy.random import normal
from sklearn.mixture import GaussianMixture
from sklearn.datasets import load_iris

data = load_iris()
X = data.data
y = data.target

# Testing 10 different instances of EM model
max_accs = []
for iter in range(0, 10):
  # fit model
  model = GaussianMixture(n_components=3, init_params='random')
  model.fit(X)

  # Testing accuracy
  results = np.array([])
  results = model.predict(X)

  accs = np.array([])
  accs = np.append(accs, sum(1 for x, y in zip(results, y) if x == y) / len(results))

  for m in range(0, 2):
    for n in range(0, 2):
      results = results - 1
      results[np.where(results == -1)[0]] = 2
      accs = np.append(accs, sum(1 for x, y in zip(results, y) if x == y) / len(results))

    # Swapping 0's and 2's to get other permutations
    if m == 0:
      results = results - 1
      results[np.where(results == 0)[0]] = 2
      results[np.where(results == -1)[0]] = 0
      accs = np.append(accs, sum(1 for x,y in zip(results,y) if x == y) / len(results))

  max_accs.append(max(accs))

print('Results over 10 EM modelings')
print('mean acc: ', np.sum(max_accs) / len(max_accs))
print('acc range: ', min(max_accs), '-', max(max_accs))